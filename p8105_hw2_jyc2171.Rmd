---
title: "Homework 2"
author: "Jerry Chao"
date: "9/24/2020"
output: github_document
---

## This is my solution to Homework 2

## Problem 0
I have created a GitHub repo as well as local R Project named "p8105_hw2_jyc2171", followed by one single .Rmd file named "p8105_hw2_jyc2171.Rmd" that renders to GitHub repo.  A subdirectory was created to use the data files Mr. Trash Wheel (Problem 1), NYC transit data (Problem 2), and FiveThiryEight (Problem 3) using relative paths.  This homework shall be submitted via courseworks as a URL to my GitHub repo.  Thanks.

```{r setup}
library(tidyverse)
library(readxl)
```

## Problem 1 - Mr. Trash Wheel dataset.  Read and clean dataset.


```{r}
trashwheel_df = 
  read_xlsx(
    "./data/Trash-Wheel-Collection-Totals-8-6-19.xlsx",
    sheet = "Mr. Trash Wheel",
    range = cell_cols("A:N")) %>% 
  janitor::clean_names() %>% 
  drop_na(dumpster) %>% 
  mutate(
    sports_balls = round(sports_balls),
    sports_balls = as.integer(sports_balls)
  )
```

Read precipitation data! for 2018 and 2017.

```{r}
precip_2018 =
  read_excel(
    "./data/Trash-Wheel-Collection-Totals-8-6-19.xlsx",
    sheet = "2018 Precipitation",
    skip = 1 
  ) %>% 
  janitor::clean_names() %>% 
  drop_na(month) %>% 
  mutate(year = 2018) %>% 
  relocate(year)
    
precip_2017 =
  read_excel(
    "./data/Trash-Wheel-Collection-Totals-8-6-19.xlsx",
    sheet = "2017 Precipitation",
    skip = 1 
  ) %>% 
  janitor::clean_names() %>% 
  drop_na(month) %>% 
  mutate(year = 2017) %>% 
  relocate(year)
```

Now combine annual precipitation.

```{r}
month_df = 
  tibble(
    month = 1:12,
    month_name = month.name
  )

precip_df = 
  bind_rows(precip_2018, precip_2017)
  
left_join(precip_df, month_df, by = "month")
```

This dataset contains information from the Mr. Trash Wheel trash collector in Baltimore, MD.  As trash enters the inner harbor, the trashwheel collects that trash, and stores it in a dumpster.   The dataset contains information on year, month, trash collected, weight and volume of trash collected, and some specific kinds of trash.  There are a total of `r nrow(trashwheel_df)` rows in our final dataset and `r ncol(trashwheel_df)` columns.  Additional data sheets are provided for the other members of the family: Professor Trash Wheel and Captain Trash Wheel.  Other sheets include monthly precipitation data from 2014 to 2019 and a homes powered note.  

The Precip 2017 and Precip 2018 sheets contain information about monthly precipitation (measured in inches) in 2017 and 2018.  The Precip 2017 dataset contains `r ncol(precip_2017)` columns and `r nrow(precip_2017)` rows.  The Precip 2018 dataset contains `r ncol(precip_2018)` columns and `r nrow(precip_2018)` rows.  The datasets were joined together in one dataset in the "precip_df"
with `r ncol(precip_df)` columns and `r nrow(precip_df)` rows.  

The total precipitation in 2018 was `r sum(pull(precip_2018, total))` inches.
The median number of sports balls in a dumpster in 2017 was `r trashwheel_df %>% filter(year == 2017) %>% pull(sports_balls) %>% median()` balls. 

For practice, as done in synchronous class session, re-running code with most updated Mr. Trashwheel data.

```{r}
trashwheel_df = 
  read_xlsx(
    "./data/Trash-Wheel-Collection-Totals-7-2020-1.xlsx",
    sheet = "Mr. Trash Wheel",
    range = cell_cols("A:N")) %>% 
  janitor::clean_names() %>% 
  drop_na(dumpster) %>% 
  mutate(
    sports_balls = round(sports_balls),
    sports_balls = as.integer(sports_balls)
  )
```

## Problem 2 - NYC Transit data!

```{r}
nyc_transit =
  read_csv("./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") %>% 
  janitor::clean_names() %>%
  select(-division, -exit_only, -staffing, -staff_hours, -ada_notes, -free_crossover, -north_south_street,               -east_west_street, -corner, -entrance_latitude, -entrance_longitude, -station_location,                         -entrance_location) %>% 
  relocate(entry, ada) %>% 
  mutate(
    entry = recode(entry, "YES" = TRUE,  "NO" = FALSE),
    route8 = as.character(route8),
    route9 = as.character(route9),
    route10 = as.character(route10),
    route11 = as.character(route11),
    entrance_type = str_to_lower(entrance_type),
    ada = str_to_lower(ada),
    entry = str_to_lower(entry),
    vending = str_to_lower(vending)
  )
```

The NYC Transit data set contains information about New York City MTA subway stations.  The variables include station names, what line, the latitude and longitude of the station, which subway routes stop at the station, whether or not there is entry to that station, the types of entry, whether they are Americans with Diasabilities Act accessible, and, finally, whether there are vending machines.  For data cleaning, I retained/selected a certain number of specified variables and dropped the rest, converted the group variable names to snake case, and changed the contents of character variables to logical variables, and changed to lowercase letters.  There are a total of `r nrow(nyc_transit)` rows and `r ncol(nyc_transit)` columns.  These data seem to be tidy because they are in long format and list all of the possible data points.  
There are a total of `r distinct(nyc_transit, station_name) %>% nrow()` distinct stations.
There are `r filter(nyc_transit, ada == 1) %>% distinct(station_name) %>% nrow()` stations that are ADA compliant (Answer is 73 but I cannot get the R markdown file to knit).
The number of station entrance/exits without vending machines that allow entrance is `r filter(nyc_transit, vending == 0 & entry == 1) %>% nrow` (Answer is 69).
Therefore, the proportion of station entrance/exits without vending machines that allow entrance is: 69/1,868 = 3.7%.

There are `r distinct(nyc_transit, route1) %>% nrow()` distinct train lines that exist (obtained by looking at distinct number of rows in the route1 variable, which should have all possible train lines listed).

```{r}
transit_tidy =
  nyc_transit %>% 
  pivot_longer(
    route1:route11,
    names_to = "route_number",
    values_to = "train_name"
  )

filter(transit_tidy, train_name == "A") %>% distinct(station_name)
```

The data has been reformatted to long, generating a new dataframe that consists of `r nrow(transit_tidy)` rows and `r ncol(transit_tidy)` columns.  There are `r filter(transit_tidy, train_name == "A") %>% distinct(station_name)` distinct stations that serve the A train.

## Problem 3 - FiveThirtyEight data!!

First, the pols-months.csv data.

```{r}
pols_months =
  read_csv("./data/fivethirtyeight_datasets/pols-month.csv") %>% 
  separate(mon, c("year", "month", "day")) %>%
  mutate(
    president = (prez_dem * 2) + prez_gop,
    president = recode(president, "2" = "dem", "1" = "gop"),
    month = recode(month, "01" = "january", "02" = "february", "03" = "march", "04" = "april", "05" =                              "may", "06" = "june", "07" = "july", "08" = "august", "09" = "september", "10" =                                "october", "11" = "november", "12" = "december") 
    ) %>% 
    select(-prez_gop, -prez_dem, -day)
```

Next, the snp.csv data.

```{r}
stock_df =
  read_csv("./data/fivethirtyeight_datasets/snp.csv") %>% 
  separate(date, c("month", "day", "year")) %>% 
  select(-day) %>%
  relocate(year) %>% 
  arrange(year) %>% 
  mutate(
    close = as.numeric(pull(stock_df, close)),
    month = recode(month, "7" = "july", "6" = "june", "5" = "may", "4" = "april", "3" = "march", "2" =                             "february", "1" = "january", "12" = "december", "11" = "november", "10" = "october", "9" =                      "september", "8" = "august")
  )
```

Finally, the unemployment data.

```{r}
unemploy_df =
  read_csv("./data/fivethirtyeight_datasets/unemployment.csv") %>% 
  janitor::clean_names() %>% 
  select(year, january = jan, february = feb, march = mar, april = apr, may, june = jun, july = jul, august =            aug, september = sep, october = oct, november = nov, december = dec) %>% 
  pivot_longer(
    january:december,
    names_to = "month",
    values_to = "unemployment"
  )
  
```

Joining the three datasets.

```{r}
stock_pols =
  left_join(pols_months, stock_df)

final_merge =
  left_join(stock_pols, unemploy_df)

## year = as.numeric(year) put into mutate
```


All three datasets have beencleaned and joined.
The "pols-month" dataset contains the number national politicians who are democratric or republican at any time.  The dataset contains `r ncol(pols_months)` columns and `r nrow(pols_months)` rows. The cleaned dataset contains the following variables: year, month, number of governers who are dems or republicans, number of senators who are dems or republicans, number of representatives who are dems or republicans, and a president variable with the political party.  The years in the dataset range from 1947 t 2015
The "snp" dataset contains the closing value of Standard & Poor's stock market index on a given month of a year.  The dataset contains `r ncol(stock_df)` columns and `r nrow(stock_df)` rows.  The years in the dataset range from 1950 to 2015.
The "unemployment" dataset contains the percentage of unemployment per month in the associated year.  The datset was converted to long format and consists of `n col(unemploy_df)` columns and `n row(unempoly_df)` rows.  The years in the dataset range from 1948 to 2015.

